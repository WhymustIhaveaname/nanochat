# Nanochat å¯èƒ½çš„ Bug åŠä¿®å¤

æœ¬æ–‡æ¡£è®°å½•äº†åœ¨ `/mnt/data/ning/code/nanochat` ä»“åº“ä¸­å‘ç°å¹¶ä¿®å¤çš„åŸç‰ˆ nanochat å¯èƒ½å­˜åœ¨çš„ bugã€‚

---

## 1. CORE Metric è¯„ä¼°å¯¼è‡´å†…å­˜æ³„æ¼ (OOM)

### é—®é¢˜æè¿°

è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¯æ¬¡ CORE metric è¯„ä¼°å GPU å†…å­˜ä¼šçªç„¶å¢åŠ ï¼Œå¯¼è‡´åç»­è®­ç»ƒæ­¥éª¤ OOMï¼ˆOut of Memoryï¼‰ã€‚

### åŸå› åˆ†æ

CORE è¯„ä¼°ä½¿ç”¨ `orig_model`ï¼ˆæœªç¼–è¯‘çš„åŸå§‹æ¨¡å‹ï¼‰ï¼Œè€Œä¸æ˜¯ `torch.compile()` ç¼–è¯‘åçš„æ¨¡å‹ã€‚`orig_model` åœ¨ forward æ—¶ä¼šåˆ›å»ºç‹¬ç«‹çš„ï¼š
- cuDNN workspace
- CUDA buffers
- ä¸­é—´æ¿€æ´»å€¼ç¼“å­˜

è¿™äº›èµ„æºä¸ç¼–è¯‘åæ¨¡å‹çš„èµ„æºæ˜¯åˆ†å¼€ç®¡ç†çš„ï¼ŒPyTorch ä¸ä¼šè‡ªåŠ¨é‡Šæ”¾å®ƒä»¬ï¼Œå¯¼è‡´å†…å­˜æŒç»­å¢é•¿ã€‚

### Bug ä»£ç  (scripts/base_train.py)

```python
# åŸç‰ˆä»£ç  - è¯„ä¼°åæœªæ¸…ç† GPU ç¼“å­˜
if master_process and (last_step or (step > 0 and step % core_metric_every == 0)):
    results = evaluate_model(orig_model, tokenizer, device, max_per_task=core_metric_max_per_task)
    # é—®é¢˜ï¼šorig_model åˆ›å»ºçš„ CUDA buffers æ²¡æœ‰è¢«é‡Šæ”¾
    # åç»­è®­ç»ƒæ—¶å†…å­˜ä¸è¶³å¯¼è‡´ OOM
```

### ä¿®å¤ä»£ç 

```python
# ä¿®å¤å - è¯„ä¼°åæ¸…ç† GPU ç¼“å­˜
if master_process and (last_step or (step > 0 and step % core_metric_every == 0)):
    # Clear CUDA cache before evaluation to free memory from training
    torch.cuda.empty_cache()

    results = evaluate_model(orig_model, tokenizer, device, max_per_task=core_metric_max_per_task)

    # Clear CUDA cache created by orig_model to prevent OOM during subsequent training
    # orig_model creates its own cuDNN workspace and buffers separate from compiled model
    torch.cuda.empty_cache()
```

### åŒæ ·çš„é—®é¢˜ä¹Ÿå‡ºç°åœ¨ Sample Generation

```python
# åŸç‰ˆä»£ç 
if master_process and (last_step or (step > 0 and step % sample_every == 0)):
    samples = generate_samples(orig_model, ...)
    # åŒæ ·çš„é—®é¢˜ï¼šæœªæ¸…ç†ç¼“å­˜

# ä¿®å¤å
if master_process and (last_step or (step > 0 and step % sample_every == 0)):
    samples = generate_samples(orig_model, ...)
    # Clear CUDA cache created by orig_model (same reason as CORE eval above)
    torch.cuda.empty_cache()
```

### å½±å“èŒƒå›´

- é•¿æ—¶é—´è®­ç»ƒï¼ˆå¤šæ¬¡è§¦å‘ CORE evalï¼‰
- å¤§æ¨¡å‹è®­ç»ƒï¼ˆæ˜¾å­˜æ¥è¿‘ä¸Šé™ï¼‰
- å¤š GPU è®­ç»ƒï¼ˆæ¯ä¸ª GPU éƒ½ä¼šç§¯ç´¯æœªé‡Šæ”¾çš„ç¼“å­˜ï¼‰

---

## 2. Rotary Embedding è¾“å‡º dtype ä¸ä¸€è‡´

### é—®é¢˜æè¿°

`apply_rotary_emb()` å‡½æ•°å¯èƒ½è¿”å›ä¸è¾“å…¥ä¸åŒçš„ dtypeï¼Œå¯¼è‡´åç»­è®¡ç®—å‡ºç°ç²¾åº¦é—®é¢˜æˆ–ç±»å‹ä¸åŒ¹é…é”™è¯¯ã€‚

### åŸå› åˆ†æ

åœ¨ `apply_rotary_emb` ä¸­ï¼Œ`cos` å’Œ `sin` å¼ é‡å¯èƒ½æ˜¯ `float32` ç±»å‹ï¼ˆå› ä¸ºé¢‘ç‡è®¡ç®—éœ€è¦é«˜ç²¾åº¦ï¼‰ï¼Œè€Œè¾“å…¥ `x` æ˜¯ `bfloat16`ã€‚è®¡ç®—è¿‡ç¨‹ä¸­ä¼šå‘ç”Ÿéšå¼ç±»å‹æå‡ï¼Œä½†è¿”å›æ—¶æ²¡æœ‰æ˜¾å¼è½¬æ¢å›åŸå§‹ç±»å‹ã€‚

### Bug ä»£ç  (nanochat/gpt.py)

```python
def apply_rotary_emb(x, cos, sin):
    """Apply rotary embeddings to x."""
    d = x.size(-1) // 2
    x1, x2 = x[..., :d], x[..., d:]  # split up last dim into two halves
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3)  # é—®é¢˜ï¼šè¿”å›çš„ dtype å¯èƒ½ä¸ x ä¸åŒ
```

### ä¿®å¤ä»£ç 

```python
def apply_rotary_emb(x, cos, sin):
    """Apply rotary embeddings to x."""
    d = x.size(-1) // 2
    x1, x2 = x[..., :d], x[..., d:]  # split up last time into two halves
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    out = torch.cat([y1, y2], 3)  # re-assemble
    out = out.to(x.dtype)  # ensure input/output dtypes match
    return out
```

### å½±å“èŒƒå›´

- æ··åˆç²¾åº¦è®­ç»ƒï¼ˆautocast ç¯å¢ƒä¸‹ï¼‰
- ä¸åŒ GPU æ¶æ„ï¼ˆdtype å¤„ç†å¯èƒ½æœ‰å·®å¼‚ï¼‰
- æ¢¯åº¦è®¡ç®—ï¼ˆdtype ä¸ä¸€è‡´å¯èƒ½å¯¼è‡´æ¢¯åº¦å¼‚å¸¸ï¼‰

---

## 3. CUDA å†…å­˜åˆ†é…ç¯å¢ƒå˜é‡åç§°é”™è¯¯

### é—®é¢˜æè¿°

ç¯å¢ƒå˜é‡åç§°æ‹¼å†™é”™è¯¯ï¼Œå¯¼è‡´ `expandable_segments` é…ç½®æœªç”Ÿæ•ˆã€‚

### åŸå› åˆ†æ

PyTorch çš„ CUDA å†…å­˜åˆ†é…å™¨é…ç½®ç¯å¢ƒå˜é‡åç§°æ˜¯ `PYTORCH_CUDA_ALLOC_CONF`ï¼Œä¸æ˜¯ `PYTORCH_ALLOC_CONF`ã€‚å°‘äº† `CUDA_` å‰ç¼€å¯¼è‡´é…ç½®è¢«å¿½ç•¥ã€‚

### Bug ä»£ç  (scripts/base_train.py)

```python
# é”™è¯¯çš„ç¯å¢ƒå˜é‡å
os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
```

### ä¿®å¤ä»£ç 

```python
# æ­£ç¡®çš„ç¯å¢ƒå˜é‡å
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
```

### å½±å“èŒƒå›´

- å†…å­˜ç¢ç‰‡é—®é¢˜ï¼š`expandable_segments` æœªå¯ç”¨ä¼šå¯¼è‡´æ›´å¤šå†…å­˜ç¢ç‰‡
- å¤§æ¨¡å‹è®­ç»ƒå¯èƒ½æ›´æ—©é‡åˆ° OOM
- å†…å­˜ä½¿ç”¨æ•ˆç‡é™ä½

### å‚è€ƒ

- [PyTorch CUDA Memory Management](https://pytorch.org/docs/stable/notes/cuda.html#memory-management)

---

## 4. ~~Generate å‡½æ•°é‡‡æ ·é€»è¾‘é—®é¢˜~~ (å·²æ¾„æ¸…ï¼šé Bug)

> âš ï¸ **æ¾„æ¸…**: ç»è¿‡ä»”ç»†åˆ†æï¼Œè¿™ä¸æ˜¯åŸç‰ˆä»£ç çš„ bugï¼Œè€Œæ˜¯ ning ç‰ˆæœ¬åœ¨é‡æ„æ—¶å¼•å…¥çš„ä¸€ä¸ª**é™çº§**ã€‚åŸç‰ˆä»£ç æ˜¯æ­£ç¡®çš„ã€‚

### ning ç‰ˆæœ¬çš„é—®é¢˜

ning åœ¨é‡æ„ `engine.py` çš„ `generate()` å‡½æ•°æ—¶ï¼Œå°† forward è°ƒç”¨ä»å¾ªç¯æœ«å°¾ç§»åˆ°äº†å¾ªç¯å¼€å¤´ï¼Œä½†ç¬¬ä¸€æ¬¡è¿­ä»£æ²¡æœ‰æ­£ç¡®å¤„ç†ï¼Œå¯¼è‡´ï¼š

**æ‰€æœ‰è¡Œçš„ç¬¬ä¸€ä¸ªç”Ÿæˆ token è¢«å¼ºåˆ¶è®¾ä¸ºç›¸åŒå€¼ï¼ˆbroadcastï¼‰**ï¼Œé™ä½äº†å¤šæ ·æœ¬ç”Ÿæˆçš„å¤šæ ·æ€§ã€‚

### åŸç‰ˆä»£ç  (æ­£ç¡® âœ…)

```python
# åŸç‰ˆ: Prefill åæ‰©å±• logits åˆ° batch size
logits = self.model.forward(ids, kv_cache=kv_cache_prefill)
logits = logits[:, -1, :].expand(num_samples, -1)  # (num_samples, vocab_size)

# While å¾ªç¯ä¸­æ¯è¡Œç‹¬ç«‹é‡‡æ ·
while True:
    next_ids = sample_next_token(logits, rng, temperature, top_k)  # (B, 1)
    sampled_tokens = next_ids[:, 0].tolist()  # æ¯è¡Œå¯ä»¥å¾—åˆ°ä¸åŒçš„ token!
    # ...å¤„ç†...
    logits = self.model.forward(ids, kv_cache=kv_cache_decode)[:, -1, :]
```

**æ•ˆæœ**: å³ä½¿ç¬¬ä¸€ä¸ª tokenï¼Œæ¯è¡Œä¹Ÿå¯ä»¥ç‹¬ç«‹é‡‡æ ·å¾—åˆ°ä¸åŒç»“æœã€‚

### ning ç‰ˆæœ¬ä»£ç  (é™çº§ âŒ)

```python
# ning ç‰ˆ: Prefill æ—¶å°±é‡‡æ ·ï¼ˆbatch=1ï¼‰
logits = self.model.forward(ids, kv_cache=kv_cache_prefill)
logits = logits[:, -1, :]  # (1, vocab) - æ²¡æœ‰æ‰©å±•!
next_ids = sample_next_token(logits, rng, temperature, top_k)
sampled_tokens = next_ids[:, 0].tolist()  # [å•ä¸ª token]

# While å¾ªç¯ä¸­ç¬¬ä¸€æ¬¡è¿­ä»£å¼ºåˆ¶ broadcast
first_iteration = True
while True:
    if first_iteration:
        sampled_tokens = [sampled_tokens[0]] * num_samples  # æ‰€æœ‰è¡Œå¼ºåˆ¶ç›¸åŒ!
        # TODO: we should sample a token for each row instead of broadcasting
        first_iteration = False
    else:
        logits = self.model.forward(ids, kv_cache=kv_cache_decode)
        # ...æ­£å¸¸é‡‡æ ·...
```

**æ•ˆæœ**: ç¬¬ä¸€ä¸ªç”Ÿæˆçš„ token æ‰€æœ‰è¡Œéƒ½ç›¸åŒï¼Œåªæœ‰åç»­ token æ‰èƒ½åˆ†å‰ã€‚

### å¯¹æ¯”ç¤ºä¾‹

å‡è®¾ `num_samples=3`, `temperature=1.0`:

| ç‰ˆæœ¬ | Row 0 | Row 1 | Row 2 |
|------|-------|-------|-------|
| **åŸç‰ˆ** | "The answer is **42**" | "The answer is **seven**" | "The answer is **unknown**" |
| **ning ç‰ˆ** | "The answer is **42**" | "The answer is **42**, but..." | "The answer is **42** or..." |

ning ç‰ˆçš„ç¬¬ä¸€ä¸ªç”Ÿæˆ token å…¨éƒ¨è¢«å¼ºåˆ¶ä¸º "42"ã€‚

### å½±å“èŒƒå›´

- **pass@k è¯„ä¼°**: å¤šæ ·æ€§é™ä½ä¼šå½±å“ pass@k æŒ‡æ ‡
- **RL è®­ç»ƒ**: `chat_rl.py` ä¸­éœ€è¦å¤šæ ·æœ¬é‡‡æ ·ï¼Œå¤šæ ·æ€§é™ä½å¯èƒ½å½±å“è®­ç»ƒæ•ˆæœ
- **ning è‡ªå·±æ„è¯†åˆ°äº†é—®é¢˜**: ç•™ä¸‹äº† TODO æ³¨é‡Š

### ç»“è®º

**ä¸å»ºè®®é‡‡ç”¨ ning ç‰ˆæœ¬çš„è¿™ä¸ªæ”¹åŠ¨**ã€‚åŸç‰ˆä»£ç æ˜¯æ­£ç¡®çš„ã€‚

---

## æ€»ç»“

| Bug | ä¸¥é‡ç¨‹åº¦ | è§¦å‘æ¡ä»¶ | ä¿®å¤éš¾åº¦ |
|-----|---------|---------|---------|
| CORE Metric å†…å­˜æ³„æ¼ | ğŸ”´ é«˜ | é•¿æ—¶é—´è®­ç»ƒ + å¤§æ¨¡å‹ | ä½ |
| Rotary Embedding dtype | ğŸŸ¡ ä¸­ | æ··åˆç²¾åº¦è®­ç»ƒ | ä½ |
| ç¯å¢ƒå˜é‡åç§°é”™è¯¯ | ğŸŸ¡ ä¸­ | æ‰€æœ‰è®­ç»ƒ | ä½ |
| Generate é‡‡æ ·é€»è¾‘ | ğŸŸ¢ ä½ | å¤šæ ·æœ¬ç”Ÿæˆ | ä¸­ |

---

## å‚è€ƒæ¥æº

- ning ä»“åº“: `/mnt/data/ning/code/nanochat`
- `ning/docs/work_log.md`: CORE Metric Memory Leak Bug
- `ning/docs/fsdp_dev_notes.md`: å¼€å‘è°ƒè¯•è®°å½•
- Git diff å¯¹æ¯”åˆ†æ
