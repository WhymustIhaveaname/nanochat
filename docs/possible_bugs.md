# Nanochat å¯èƒ½çš„ Bug åŠä¿®å¤

æœ¬æ–‡æ¡£è®°å½•äº†åœ¨ `/mnt/data/ning/code/nanochat` ä»“åº“ä¸­å‘ç°å¹¶ä¿®å¤çš„åŸç‰ˆ nanochat å¯èƒ½å­˜åœ¨çš„ bugã€‚

---

## 1. CORE Metric è¯„ä¼°å¯¼è‡´å†…å­˜æ³„æ¼ (OOM)

### é—®é¢˜æè¿°

è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¯æ¬¡ CORE metric è¯„ä¼°å GPU å†…å­˜ä¼šçªç„¶å¢åŠ ï¼Œå¯¼è‡´åç»­è®­ç»ƒæ­¥éª¤ OOMï¼ˆOut of Memoryï¼‰ã€‚

### åŸå› åˆ†æ

CORE è¯„ä¼°ä½¿ç”¨ `orig_model`ï¼ˆæœªç¼–è¯‘çš„åŸå§‹æ¨¡å‹ï¼‰ï¼Œè€Œä¸æ˜¯ `torch.compile()` ç¼–è¯‘åçš„æ¨¡å‹ã€‚`orig_model` åœ¨ forward æ—¶ä¼šåˆ›å»ºç‹¬ç«‹çš„ï¼š
- cuDNN workspace
- CUDA buffers
- ä¸­é—´æ¿€æ´»å€¼ç¼“å­˜

è¿™äº›èµ„æºä¸ç¼–è¯‘åæ¨¡å‹çš„èµ„æºæ˜¯åˆ†å¼€ç®¡ç†çš„ï¼ŒPyTorch ä¸ä¼šè‡ªåŠ¨é‡Šæ”¾å®ƒä»¬ï¼Œå¯¼è‡´å†…å­˜æŒç»­å¢é•¿ã€‚

### Bug ä»£ç  (scripts/base_train.py)

```python
# åŸç‰ˆä»£ç  - è¯„ä¼°åæœªæ¸…ç† GPU ç¼“å­˜
if master_process and (last_step or (step > 0 and step % core_metric_every == 0)):
    results = evaluate_model(orig_model, tokenizer, device, max_per_task=core_metric_max_per_task)
    # é—®é¢˜ï¼šorig_model åˆ›å»ºçš„ CUDA buffers æ²¡æœ‰è¢«é‡Šæ”¾
    # åç»­è®­ç»ƒæ—¶å†…å­˜ä¸è¶³å¯¼è‡´ OOM
```

### ä¿®å¤ä»£ç 

```python
# ä¿®å¤å - è¯„ä¼°åæ¸…ç† GPU ç¼“å­˜
if master_process and (last_step or (step > 0 and step % core_metric_every == 0)):
    # Clear CUDA cache before evaluation to free memory from training
    torch.cuda.empty_cache()

    results = evaluate_model(orig_model, tokenizer, device, max_per_task=core_metric_max_per_task)

    # Clear CUDA cache created by orig_model to prevent OOM during subsequent training
    # orig_model creates its own cuDNN workspace and buffers separate from compiled model
    torch.cuda.empty_cache()
```

### åŒæ ·çš„é—®é¢˜ä¹Ÿå‡ºç°åœ¨ Sample Generation

```python
# åŸç‰ˆä»£ç 
if master_process and (last_step or (step > 0 and step % sample_every == 0)):
    samples = generate_samples(orig_model, ...)
    # åŒæ ·çš„é—®é¢˜ï¼šæœªæ¸…ç†ç¼“å­˜

# ä¿®å¤å
if master_process and (last_step or (step > 0 and step % sample_every == 0)):
    samples = generate_samples(orig_model, ...)
    # Clear CUDA cache created by orig_model (same reason as CORE eval above)
    torch.cuda.empty_cache()
```

### å½±å“èŒƒå›´

- é•¿æ—¶é—´è®­ç»ƒï¼ˆå¤šæ¬¡è§¦å‘ CORE evalï¼‰
- å¤§æ¨¡å‹è®­ç»ƒï¼ˆæ˜¾å­˜æ¥è¿‘ä¸Šé™ï¼‰
- å¤š GPU è®­ç»ƒï¼ˆæ¯ä¸ª GPU éƒ½ä¼šç§¯ç´¯æœªé‡Šæ”¾çš„ç¼“å­˜ï¼‰

---

## 2. Rotary Embedding è¾“å‡º dtype ä¸ä¸€è‡´

### é—®é¢˜æè¿°

`apply_rotary_emb()` å‡½æ•°å¯èƒ½è¿”å›ä¸è¾“å…¥ä¸åŒçš„ dtypeï¼Œå¯¼è‡´åç»­è®¡ç®—å‡ºç°ç²¾åº¦é—®é¢˜æˆ–ç±»å‹ä¸åŒ¹é…é”™è¯¯ã€‚

### åŸå› åˆ†æ

åœ¨ `apply_rotary_emb` ä¸­ï¼Œ`cos` å’Œ `sin` å¼ é‡å¯èƒ½æ˜¯ `float32` ç±»å‹ï¼ˆå› ä¸ºé¢‘ç‡è®¡ç®—éœ€è¦é«˜ç²¾åº¦ï¼‰ï¼Œè€Œè¾“å…¥ `x` æ˜¯ `bfloat16`ã€‚è®¡ç®—è¿‡ç¨‹ä¸­ä¼šå‘ç”Ÿéšå¼ç±»å‹æå‡ï¼Œä½†è¿”å›æ—¶æ²¡æœ‰æ˜¾å¼è½¬æ¢å›åŸå§‹ç±»å‹ã€‚

### Bug ä»£ç  (nanochat/gpt.py)

```python
def apply_rotary_emb(x, cos, sin):
    """Apply rotary embeddings to x."""
    d = x.size(-1) // 2
    x1, x2 = x[..., :d], x[..., d:]  # split up last dim into two halves
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3)  # é—®é¢˜ï¼šè¿”å›çš„ dtype å¯èƒ½ä¸ x ä¸åŒ
```

### ä¿®å¤ä»£ç 

```python
def apply_rotary_emb(x, cos, sin):
    """Apply rotary embeddings to x."""
    d = x.size(-1) // 2
    x1, x2 = x[..., :d], x[..., d:]  # split up last time into two halves
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    out = torch.cat([y1, y2], 3)  # re-assemble
    out = out.to(x.dtype)  # ensure input/output dtypes match
    return out
```

### å½±å“èŒƒå›´

- æ··åˆç²¾åº¦è®­ç»ƒï¼ˆautocast ç¯å¢ƒä¸‹ï¼‰
- ä¸åŒ GPU æ¶æ„ï¼ˆdtype å¤„ç†å¯èƒ½æœ‰å·®å¼‚ï¼‰
- æ¢¯åº¦è®¡ç®—ï¼ˆdtype ä¸ä¸€è‡´å¯èƒ½å¯¼è‡´æ¢¯åº¦å¼‚å¸¸ï¼‰

---

## 3. CUDA å†…å­˜åˆ†é…ç¯å¢ƒå˜é‡åç§°é”™è¯¯

### é—®é¢˜æè¿°

ç¯å¢ƒå˜é‡åç§°æ‹¼å†™é”™è¯¯ï¼Œå¯¼è‡´ `expandable_segments` é…ç½®æœªç”Ÿæ•ˆã€‚

### åŸå› åˆ†æ

PyTorch çš„ CUDA å†…å­˜åˆ†é…å™¨é…ç½®ç¯å¢ƒå˜é‡åç§°æ˜¯ `PYTORCH_CUDA_ALLOC_CONF`ï¼Œä¸æ˜¯ `PYTORCH_ALLOC_CONF`ã€‚å°‘äº† `CUDA_` å‰ç¼€å¯¼è‡´é…ç½®è¢«å¿½ç•¥ã€‚

### Bug ä»£ç  (scripts/base_train.py)

```python
# é”™è¯¯çš„ç¯å¢ƒå˜é‡å
os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
```

### ä¿®å¤ä»£ç 

```python
# æ­£ç¡®çš„ç¯å¢ƒå˜é‡å
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
```

### å½±å“èŒƒå›´

- å†…å­˜ç¢ç‰‡é—®é¢˜ï¼š`expandable_segments` æœªå¯ç”¨ä¼šå¯¼è‡´æ›´å¤šå†…å­˜ç¢ç‰‡
- å¤§æ¨¡å‹è®­ç»ƒå¯èƒ½æ›´æ—©é‡åˆ° OOM
- å†…å­˜ä½¿ç”¨æ•ˆç‡é™ä½

### å‚è€ƒ

- [PyTorch CUDA Memory Management](https://pytorch.org/docs/stable/notes/cuda.html#memory-management)

---

## æ€»ç»“

| Bug | ä¸¥é‡ç¨‹åº¦ | è§¦å‘æ¡ä»¶ | ä¿®å¤éš¾åº¦ |
|-----|---------|---------|---------|
| CORE Metric å†…å­˜æ³„æ¼ | ğŸ”´ é«˜ | é•¿æ—¶é—´è®­ç»ƒ + å¤§æ¨¡å‹ | ä½ |
| Rotary Embedding dtype | ğŸŸ¡ ä¸­ | æ··åˆç²¾åº¦è®­ç»ƒ | ä½ |
| ç¯å¢ƒå˜é‡åç§°é”™è¯¯ | ğŸŸ¡ ä¸­ | æ‰€æœ‰è®­ç»ƒ | ä½ |

---

## å‚è€ƒæ¥æº

- ning ä»“åº“: `/mnt/data/ning/code/nanochat`
- `ning/docs/work_log.md`: CORE Metric Memory Leak Bug
- `ning/docs/fsdp_dev_notes.md`: å¼€å‘è°ƒè¯•è®°å½•
- Git diff å¯¹æ¯”åˆ†æ
